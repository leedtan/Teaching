{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rF2SjfaemTam"
   },
   "source": [
    "#CNN on CIFAR-10\n",
    "\n",
    "\n",
    "\n",
    "CIFAR-10 dataset contains 32x32 color images from 10 classes: __airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck__:\n",
    "<img src=\"https://github.com/hse-aml/intro-to-dl/blob/master/week3/images/cifar10.jpg?raw=1\" style=\"width:80%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "hEsvDLHTmTax"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QxoLKqETmTa7",
    "outputId": "b7f92729-5d73-4eba-bd81-8b8936d00e42"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170500096/170498071 [==============================] - 23s 0us/step\n",
      "170508288/170498071 [==============================] - 23s 0us/step\n",
      "Train samples: (50000, 32, 32, 3) (50000, 1)\n",
      "Test samples: (10000, 32, 32, 3) (10000, 1)\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import cifar10\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "print(\"Train samples:\", x_train.shape, y_train.shape)\n",
    "print(\"Test samples:\", x_test.shape, y_test.shape)\n",
    "NUM_CLASSES = 10\n",
    "cifar10_classes = [\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", \n",
    "                   \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"]\n",
    "\n",
    "x_mean = np.mean(x_train, axis=(0, 1,2),keepdims=True)\n",
    "x_std = np.std(x_train, axis=(0, 1,2),keepdims=True)\n",
    "x_train_norm = (x_train - x_mean) / x_std\n",
    "x_test_norm = (x_test - x_mean) / x_std\n",
    "y_train_onehot = pd.get_dummies(y_train.squeeze()).values\n",
    "y_test_onehot = pd.get_dummies(y_test.squeeze()).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xGKudETvmTbD"
   },
   "outputs": [],
   "source": [
    "# show random images from train\n",
    "cols = 8\n",
    "rows = 2\n",
    "fig = plt.figure(figsize=(2 * cols - 1, 2.5 * rows - 1))\n",
    "for i in range(cols):\n",
    "    for j in range(rows):\n",
    "        random_index = np.random.randint(0, len(y_train))\n",
    "        ax = fig.add_subplot(rows, cols, i * rows + j + 1)\n",
    "        ax.grid('off')\n",
    "        ax.axis('off')\n",
    "        ax.imshow(x_train[random_index, :])\n",
    "        ax.set_title(cifar10_classes[y_train[random_index, 0]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "11QYFYJDt-fN"
   },
   "outputs": [],
   "source": [
    "\n",
    "from functools import partial\n",
    "tfph = tf.compat.v1.placeholder\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "class ResidualConv:\n",
    "  def __init__(self, layer_sizes):\n",
    "    self.layers = [partial(tf.compat.v1.layers.conv2d, filters = size, kernel_size = 3, padding = 'SAME') for size in layer_sizes]\n",
    "  def forward(self, signal):\n",
    "    x = signal\n",
    "    self.features = []\n",
    "    for i, layer in enumerate(self.layers):\n",
    "      x = layer(x)\n",
    "      if i < len(self.layers) - 1:\n",
    "        x = tf.nn.leaky_relu(x)\n",
    "      self.features.append(x)\n",
    "    self.output = signal + x\n",
    "    return self.output\n",
    "\n",
    "class Conv:\n",
    "  def __init__(self, *args, **kwargs):\n",
    "    self.layer = partial(tf.compat.v1.layers.conv2d, *args, **kwargs)\n",
    "  def forward(self, signal):\n",
    "    return self.layer(signal)\n",
    "class BaseConv(Conv):\n",
    "  def __init__(self, size):\n",
    "    super().__init__(filters = size, kernel_size = 3, padding='SAME')\n",
    "class StridedConv(Conv):\n",
    "  def __init__(self, size):\n",
    "    super().__init__(filters = size, kernel_size = 5, strides = 2, padding='SAME')\n",
    "BC, SC, RC = BaseConv, StridedConv, ResidualConv\n",
    "\n",
    "# layers = [BC(32)] * 2 + [SC(64)] + [BC(64)] * 2 + [SC(128)] + [BC(128)] * 2 # What's the bug here?\n",
    "# layers = [BC(32), BC(32), SC(64), BC(64), BC(64), SC(128), BC(128), BC(128)] # .68\n",
    "\n",
    "layers = [BC(32), RC([16, 32]), RC([16, 32]), SC(64), RC([32, 64]), RC([32, 64]), SC(128), RC([64, 128]), RC([64, 128])] # .70\n",
    "# layers = [BC(32),BC(32), RC([16, 32]), RC([16, 32]), RC([16, 32]), RC([16, 32]), SC(64),\n",
    "#           RC([32, 64]), RC([32, 64]), RC([32, 64]), RC([32, 64]),\n",
    "#           SC(128), RC([64, 128]), RC([64, 128]), RC([64, 128]), RC([64, 128]), RC([64, 128]), RC([64, 128]),\n",
    "#           RC([64, 128]), RC([64, 128]), RC([64, 128]), RC([64, 128])]\n",
    "# batch_size = 128 # .69\n",
    "batch_size = 64 # .71\n",
    "class Model:\n",
    "  def __init__(self, layers = layers, extra_loss_layers = [SC(64), SC(128)]):\n",
    "    self.xph = tfph(tf.float32, shape = (None, 32, 32, 3))\n",
    "    self.yph = tfph(tf.int32, shape = (None, NUM_CLASSES))\n",
    "    self.layers = layers\n",
    "    self.features = [self.xph]\n",
    "    for i, layer in enumerate(layers):\n",
    "      features = layer.forward(self.features[-1])\n",
    "      features = tf.nn.leaky_relu(features)\n",
    "      additional_layer = tf.compat.v1.placeholder_with_default(tf.float32, shape = tf.shape(features))\n",
    "      features = features + additional_layer\n",
    "      self.features.append(features)\n",
    "      if len(layers) // 2 == i:\n",
    "        pivot_features = features\n",
    "        for j, extra_layer in enumerate(extra_loss_layers):\n",
    "          pivot_feautres = layer.forward(pivot_features)\n",
    "          pivot_feautres = tf.nn.leaky_relu(pivot_feautres)\n",
    "        pivot_feautres = tf.compat.v1.layers.flatten(pivot_feautres)\n",
    "        extra_pred = tf.compat.v1.layers.dense(pivot_feautres, NUM_CLASSES)\n",
    "        self.extra_loss = tf.compat.v1.losses.softmax_cross_entropy(self.yph, extra_pred)\n",
    "    features = tf.compat.v1.layers.flatten(features)\n",
    "    features = tf.compat.v1.layers.dense(features, 64)\n",
    "    features = tf.nn.leaky_relu(features)\n",
    "    self.features.append(features)\n",
    "    output_raw = tf.compat.v1.layers.dense(features, NUM_CLASSES)\n",
    "    self.features.append(output_raw)\n",
    "    self.yhat = tf.compat.v1.math.softmax(output_raw, axis=1)\n",
    "    self.celoss = tf.compat.v1.losses.softmax_cross_entropy(self.yph, output_raw)\n",
    "    tfvars = tf.compat.v1.get_trainable_variables()\n",
    "    self.reg = tf.reduce_sum([tf.reduce_sum(tf.square(var)) for var in tfvars])\n",
    "    self.loss = self.celoss + self.extra_loss * .2 + self.reg * 1e-8\n",
    "    self.opt = tf.compat.v1.train.AdamOptimizer(1e-4).minimize(self.loss)\n",
    "    self.sess = tf.compat.v1.Session()\n",
    "    self.sess.run(tf.compat.v1.global_variables_initializer())\n",
    "    self.losses = []\n",
    "    self.val_losses = []\n",
    "    self.val_acc = []\n",
    "    self.val_fd = {self.xph: x_test_norm, self.yph: y_test_onehot}\n",
    "    self.grad_x = tf.gradients(self.loss, self.xph)[0]\n",
    "    \n",
    "  def train(self, x_test_norm, y_test_onehot, steps = 10000,batch_size = 64):\n",
    "    for step in range(steps):\n",
    "      samples = np.random.choice(x_train.shape[0], batch_size)\n",
    "      x_sample = x_train_norm[samples]\n",
    "      if step % 3 == 0:\n",
    "        noise = np.random.randn(*x_train_norm[samples].shape) * 1e-2\n",
    "        x_trn = x_sample + noise\n",
    "      elif step % 3 == 1:\n",
    "        self.fd = {self.xph: x_sample, self.yph: y_train_onehot[samples]}\n",
    "        grad_x = self.sess.run(self.grad_x, self.fd)\n",
    "        x_trn = x_sample + (grad_x > 0) * 1e-2 - 1e-2/2\n",
    "      else:\n",
    "        self.fd = {self.xph: x_sample, self.yph: y_train_onehot[samples]}\n",
    "        grad_x = self.sess.run(self.grad_x, self.fd)\n",
    "        grad_x = np.sqrt(np.abs(grad_x)) * ((grad_x > 0) * 2 - 1)\n",
    "        norm = np.sqrt(np.sum(np.square(grad_x).reshape(-1, 32*32*3), axis=1))\n",
    "        distortion = grad_x / norm[:,None,None,None]\n",
    "        x_trn = x_sample + distortion\n",
    "\n",
    "      self.fd = {self.xph: x_trn, self.yph: y_train_onehot[samples]}\n",
    "      ls, _ = self.sess.run([self.loss, self.opt], self.fd)\n",
    "      self.losses.append(ls)\n",
    "      if step % 100 == 0:\n",
    "        val_loss, forecast = self.sess.run([self.loss, self.yhat], self.val_fd)\n",
    "        actual_pred = np.argmax(forecast, axis=1)\n",
    "        acc = (actual_pred == y_test.flatten()).mean()\n",
    "        self.val_acc.append(acc)\n",
    "        self.val_losses.append(val_loss)\n",
    "        print(val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kUtEp5gcBPxz"
   },
   "outputs": [],
   "source": [
    "\n",
    "# grad = np.random.randn(64, 32, 32, 3)\n",
    "# grad.sum(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3QTTADiO-0zN"
   },
   "outputs": [],
   "source": [
    "# batch norm:\n",
    "# layer has shape (bs, h, w, filters)\n",
    "# for each h, w, filters, there are bs of them.\n",
    "# take those bs values, and force them to be normally distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3PDjmge89yPW"
   },
   "outputs": [],
   "source": [
    "# grad_x = [-1e10, + .00000001, 10, -12]\n",
    "# every image, the l2 norm of the change, lets set that to .1\n",
    "# every image, sqrt the l2 norm of the change, lets set that to .1\n",
    "# used = [-.005, .005, .005, -.005]\n",
    "\n",
    "# other image transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SOcCHKna1yZT"
   },
   "outputs": [],
   "source": [
    "mdl = Model()\n",
    "mdl.train(x_test_norm, y_test_onehot, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7arX_G2iyOFM"
   },
   "outputs": [],
   "source": [
    "np.mean(mdl.losses[-10:]), np.min(mdl.val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kNXbB6anyHW6"
   },
   "outputs": [],
   "source": [
    "# (0.068675384, 0.9072602)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nJW-MkX93rFa"
   },
   "outputs": [],
   "source": [
    "plt.plot(mdl.losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YnchF6e-3z-W"
   },
   "outputs": [],
   "source": [
    "plt.plot(mdl.val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4WUC94mw9LgX"
   },
   "outputs": [],
   "source": [
    "\n",
    "mdl.val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xgoKU1944ArQ"
   },
   "outputs": [],
   "source": [
    "plt.plot(mdl.val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yc7c3ePwt_5G"
   },
   "outputs": [],
   "source": [
    "# validate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p-0ZSWOFs_Ga"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
