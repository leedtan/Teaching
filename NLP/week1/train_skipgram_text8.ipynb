{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skip-gram Word2Vec\n",
    "\n",
    "In this notebook, I'll lead you through using PyTorch to implement the [Word2Vec algorithm](https://en.wikipedia.org/wiki/Word2vec) using the skip-gram architecture. By implementing this, you'll learn about embedding words for use in natural language processing. This will come in handy when dealing with things like machine translation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Loading Data\n",
    "\n",
    "Next, we'll ask you to load in data and place it in the `data` directory. we'll be leveraging 100 MB of cleaned text from Wikipedia\n",
    "\n",
    "1. Load the [text8 dataset](https://s3.amazonaws.com/video.udacity-data.com/topher/2018/October/5bbe6499_text8/text8.zip); a file of cleaned up *Wikipedia article text* from Matt Mahoney. \n",
    "2. Place that data in the `data` folder in the home directory.\n",
    "3. Then you can extract it and delete the archive, zip file to save storage space.\n",
    "\n",
    "After following these steps, you should have one file in your data directory: `data/text8`.\n",
    "\n",
    "(Sorry that i didn't automate this for you, I know I could have)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the extracted text file      \n",
    "with open('data/text8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# print out the first 100 characters\n",
    "print(text[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing\n",
    "\n",
    "Here I'm fixing up the text to make training easier. This comes from the `utils.py` file. The `preprocess` function does a few things:\n",
    ">* It converts any punctuation into tokens, so a period is changed to ` <PERIOD> `. In this data set, there aren't any periods, but it will help in other NLP problems. \n",
    "* It removes all words that show up five or *fewer* times in the dataset. This will greatly reduce issues due to noise in the data and improve the quality of the vector representations. \n",
    "* It returns a list of words in the text.\n",
    "\n",
    "This may take a few seconds to run, since our text file is quite large. If you want to write your own functions for this stuff, go for it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "\n",
    "# get list of words\n",
    "words = utils.preprocess(text)\n",
    "print(words[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print some stats about this word data\n",
    "print(\"Total words in text: {}\".format(len(words)))\n",
    "print(\"Unique words: {}\".format(len(set(words)))) # `set` removes any duplicate words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dictionaries\n",
    "\n",
    "Next, I'm creating two dictionaries to convert words to integers and back again (integers to words). This is again done with a function in the `utils.py` file. `create_lookup_tables` takes in a list of words in a text and returns two dictionaries.\n",
    ">* The integers are assigned in descending frequency order, so the most frequent word (\"the\") is given the integer 0 and the next most frequent is 1, and so on. \n",
    "\n",
    "Once we have our dictionaries, the words are converted to integers and stored in the list `int_words`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_en = set(stopwords.words('english'))\n",
    "train_words = [wd for wd in words if wd not in stop_en]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "ctr = Counter(train_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist([min(c, 200) for c in ctr.values()], bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_to_int, int_to_vocab = utils.create_lookup_tables(train_words)\n",
    "int_words = [vocab_to_int[word] for word in train_words]\n",
    "\n",
    "if 0:\n",
    "    # This filtering appears already done for us with min = 6\n",
    "    word_counts = Counter(int_words)\n",
    "\n",
    "    total_count = len(int_words)\n",
    "    counts_raw = {word: count for word, count in word_counts.items()}\n",
    "    int_words = [word for word in int_words if counts_raw[word] > 5]\n",
    "\n",
    "word_counts = Counter(int_words)\n",
    "total_count = len(int_words)\n",
    "freqs = {word: count/total_count for word, count in word_counts.items()}\n",
    "\n",
    "len(int_words), len(train_words), len(int_words)/len(train_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_to_id = {idx:id for idx, id in enumerate(int_words)}\n",
    "train_length = len(int_words)\n",
    "n_window = 3\n",
    "n_vocab = len(vocab_to_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class SkipGram(nn.Module):\n",
    "    def __init__(self, n_vocab, n_embed):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.center_emb = nn.Embedding(n_vocab, n_embed)\n",
    "        self.outer_emb = nn.Embedding(n_vocab, n_embed)\n",
    "#         self.log_softmax = nn.LogSoftmax(dim=1)\n",
    "    \n",
    "    def forward(self, x_center, x_outer, target):\n",
    "        xc = self.center_emb(x_center)\n",
    "        xo = self.outer_emb(x_outer)\n",
    "        scores = torch.sum(xc * xo, axis=1)\n",
    "        probs = torch.sigmoid(scores)\n",
    "        \n",
    "        return scores, probs\n",
    "    \n",
    "    def get_emb(self):\n",
    "        return (self.outer_emb.weight + self.center_emb.weight).data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neg = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = Counter(int_words)\n",
    "total_count = len(int_words)\n",
    "freqs = {word: count/total_count for word, count in word_counts.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wd_with_p = [wd for wd in freqs.keys()]\n",
    "p_per_wd = [freqs[wd] ** (3/4) for wd in wd_with_p]\n",
    "p_sum = sum(p_per_wd)\n",
    "p_per_wd = [p/p_sum for p in p_per_wd]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_bs = 32\n",
    "xi_center = np.random.choice(train_length - n_window * 2, pos_bs, replace=False) + n_window\n",
    "xi_outer = (np.random.choice(n_window, pos_bs)+1) * np.random.choice([-1, 1], pos_bs) + xi_center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%timeit np.take(int_words, xi_center)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How much faster would it be if we vecttorized it with a dictionary instead?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_getter = np.vectorize(idx_to_id.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit id_getter(xi_center)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xpos_center = id_getter(xi_center)\n",
    "xpos_outer = id_getter(xi_outer)\n",
    "\n",
    "xneg_center = np.random.choice(wd_with_p, size = pos_bs * n_neg, replace=False, p = p_per_wd)\n",
    "xneg_outer = np.random.choice(wd_with_p, size = pos_bs * n_neg, replace=False, p = p_per_wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(wd_with_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xneg_outer.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(embedding, valid_size=16, valid_window=100, device='cpu'):\n",
    "    \"\"\" Returns the cosine similarity of validation words with words in the embedding matrix.\n",
    "        Here, embedding should be a PyTorch embedding module.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Here we're calculating the cosine similarity between some random words and \n",
    "    # our embedding vectors. With the similarities, we can look at what words are\n",
    "    # close to our random words.\n",
    "    \n",
    "    # sim = (a . b) / |a||b|\n",
    "    \n",
    "    embed_vectors = embedding.weight\n",
    "    \n",
    "    # magnitude of embedding vectors, |b|\n",
    "    magnitudes = embed_vectors.pow(2).sum(dim=1).sqrt().unsqueeze(0)\n",
    "    \n",
    "    # pick N words from our ranges (0,window) and (1000,1000+window). lower id implies more frequent \n",
    "    valid_examples = np.array(random.sample(range(valid_window), valid_size//2))\n",
    "    valid_examples = np.append(valid_examples,\n",
    "                               random.sample(range(1000,1000+valid_window), valid_size//2))\n",
    "    valid_examples = torch.LongTensor(valid_examples).to(device)\n",
    "    \n",
    "    valid_vectors = embedding(valid_examples)\n",
    "    similarities = torch.mm(valid_vectors, embed_vectors.t())/magnitudes\n",
    "        \n",
    "    return valid_examples, similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if GPU is available\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "embedding_dim= 50 # you can change, if you want\n",
    "\n",
    "model = SkipGram(len(vocab_to_int), embedding_dim).to(device)\n",
    "criterion = nn.NLLLoss()#weight = torch.Tensor([1/n_neg, 1]))\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.003)\n",
    "\n",
    "print_every = 30\n",
    "steps = 0\n",
    "epochs = 5\n",
    "\n",
    "for step in range(epochs * train_length):\n",
    "    xi_center = np.random.choice(train_length - n_window * 2, pos_bs, replace=False) + n_window\n",
    "    xi_outer = (np.random.choice(n_window, pos_bs)+1) * np.random.choice([-1, 1], pos_bs) + xi_center\n",
    "    xneg_center = np.random.choice(wd_with_p, size = pos_bs * n_neg, replace=False, p = p_per_wd)\n",
    "    xneg_outer = np.random.choice(wd_with_p, size = pos_bs * n_neg, replace=False, p = p_per_wd)\n",
    "    x_center = np.concatenate((id_getter(xi_center), xneg_center))\n",
    "    x_outer = np.concatenate((id_getter(xi_outer), xneg_outer))\n",
    "    targets = np.array([1] * len(xi_center) + [0] * len(xneg_center))\n",
    "    steps += 1\n",
    "    x_center_torch, x_outer_torch, targets_torch = [torch.LongTensor(x).to(device) for x in [x_center, x_outer, targets]]\n",
    "    \n",
    "    scores, log_ps = model(x_center_torch, x_outer_torch, targets_torch)\n",
    "    loss = criterion(log_ps, targets_torch)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if step % print_every == 0:\n",
    "        print(loss)\n",
    "        # getting examples and similarities\n",
    "        embed = model.get_emb()\n",
    "#         valid_examples, valid_similarities = cosine_similarity(model.get_emb(), device=device)\n",
    "#         _, closest_idxs = valid_similarities.topk(6) # topk highest similarities\n",
    "\n",
    "#         valid_examples, closest_idxs = valid_examples.to('cpu'), closest_idxs.to('cpu')\n",
    "#         for ii, valid_idx in enumerate(valid_examples):\n",
    "#             closest_words = [int_to_vocab[idx.item()] for idx in closest_idxs[ii]][1:]\n",
    "#             print(int_to_vocab[valid_idx.item()] + \" | \" + ', '.join(closest_words))\n",
    "#         print(\"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the word vectors\n",
    "\n",
    "Below we'll use T-SNE to visualize how our high-dimensional word vectors cluster together. T-SNE is used to project these vectors into two dimensions while preserving local stucture. Check out [this post from Christopher Olah](http://colah.github.io/posts/2014-10-Visualizing-MNIST/) to learn more about T-SNE and other ways to visualize high-dimensional data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting embeddings from the embedding layer of our model, by name\n",
    "embeddings = model.embed.weight.to('cpu').data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz_words = 600\n",
    "tsne = TSNE()\n",
    "embed_tsne = tsne.fit_transform(embeddings[:viz_words, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16, 16))\n",
    "for idx in range(viz_words):\n",
    "    plt.scatter(*embed_tsne[idx, :], color='steelblue')\n",
    "    plt.annotate(int_to_vocab[idx], (embed_tsne[idx, 0], embed_tsne[idx, 1]), alpha=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
