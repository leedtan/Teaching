{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skip-gram Word2Vec\n",
    "\n",
    "In this notebook, I'll lead you through using PyTorch to implement the [Word2Vec algorithm](https://en.wikipedia.org/wiki/Word2vec) using the skip-gram architecture. By implementing this, you'll learn about embedding words for use in natural language processing. This will come in handy when dealing with things like machine translation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Loading Data\n",
    "\n",
    "Next, we'll ask you to load in data and place it in the `data` directory. we'll be leveraging 100 MB of cleaned text from Wikipedia\n",
    "\n",
    "1. Load the [text8 dataset](https://s3.amazonaws.com/video.udacity-data.com/topher/2018/October/5bbe6499_text8/text8.zip); a file of cleaned up *Wikipedia article text* from Matt Mahoney. \n",
    "2. Place that data in the `data` folder in the home directory.\n",
    "3. Then you can extract it and delete the archive, zip file to save storage space.\n",
    "\n",
    "After following these steps, you should have one file in your data directory: `data/text8`.\n",
    "\n",
    "(Sorry that i didn't automate this for you, I know I could have)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the extracted text file      \n",
    "with open('data/text8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# print out the first 100 characters\n",
    "print(text[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing\n",
    "\n",
    "Here I'm fixing up the text to make training easier. This comes from the `utils.py` file. The `preprocess` function does a few things:\n",
    ">* It converts any punctuation into tokens, so a period is changed to ` <PERIOD> `. In this data set, there aren't any periods, but it will help in other NLP problems. \n",
    "* It removes all words that show up five or *fewer* times in the dataset. This will greatly reduce issues due to noise in the data and improve the quality of the vector representations. \n",
    "* It returns a list of words in the text.\n",
    "\n",
    "This may take a few seconds to run, since our text file is quite large. If you want to write your own functions for this stuff, go for it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import utils\n",
    "\n",
    "# get list of words\n",
    "words = utils.preprocess(text)\n",
    "print(words[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print some stats about this word data\n",
    "print(\"Total words in text: {}\".format(len(words)))\n",
    "print(\"Unique words: {}\".format(len(set(words)))) # `set` removes any duplicate words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dictionaries\n",
    "\n",
    "Next, I'm creating two dictionaries to convert words to integers and back again (integers to words). This is again done with a function in the `utils.py` file. `create_lookup_tables` takes in a list of words in a text and returns two dictionaries.\n",
    ">* The integers are assigned in descending frequency order, so the most frequent word (\"the\") is given the integer 0 and the next most frequent is 1, and so on. \n",
    "\n",
    "Once we have our dictionaries, the words are converted to integers and stored in the list `int_words`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_en = set(stopwords.words('english'))\n",
    "train_words = [wd for wd in words if wd not in stop_en]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "ctr = Counter(train_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist([min(c, 200) for c in ctr.values()], bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_to_int, int_to_vocab = utils.create_lookup_tables(train_words)\n",
    "int_words = [vocab_to_int[word] for word in train_words]\n",
    "\n",
    "if 0:\n",
    "    # This filtering appears already done for us with min = 6\n",
    "    word_counts = Counter(int_words)\n",
    "\n",
    "    total_count = len(int_words)\n",
    "    counts_raw = {word: count for word, count in word_counts.items()}\n",
    "    int_words = [word for word in int_words if counts_raw[word] > 5]\n",
    "\n",
    "word_counts = Counter(int_words)\n",
    "total_count = len(int_words)\n",
    "freqs = {word: count/total_count for word, count in word_counts.items()}\n",
    "\n",
    "len(int_words), len(train_words), len(int_words)/len(train_words)\n",
    "int_words = np.array(int_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_to_id = {idx:id for idx, id in enumerate(int_words)}\n",
    "train_length = len(int_words)\n",
    "n_window = 3\n",
    "n_vocab = len(vocab_to_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class SkipGram(nn.Module):\n",
    "    def __init__(self, n_vocab, n_embed):\n",
    "        super().__init__()\n",
    "        # TODO: Initialize model\n",
    "#         self.center_emb = \n",
    "#         self.outer_emb =\n",
    "    \n",
    "    def forward(self, x_center, x_outer, target):\n",
    "        #TODO make predictions (raw scores and probabilities)\n",
    "        return scores, probs\n",
    "    \n",
    "    def get_emb(self):\n",
    "        #TODO get embeddings from model\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neg = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = Counter(int_words)\n",
    "total_count = len(int_words)\n",
    "freqs = {word: count/total_count for word, count in word_counts.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wd_with_p = # Create ordered list of unique words\n",
    "p_per_wd = # Get 3/4 power unigram probability of word counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_bs = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "def cosine_similarity_vec(u, v):\n",
    "    # TODO: Calculate cosine similarity\n",
    "    return cos\n",
    "def test_some_similarities(embed):\n",
    "    word_pairs = [\n",
    "                    ['father', 'mother'], ['boy', 'girl'], ['football', 'baseball'], ['good', 'bad'],\n",
    "        ['accurate', 'precise'], ['allude','refer'],['anxious','eager'],['convince','persuade'],\n",
    "        ['fewer','less'],['poisonous','venomous']\n",
    "                ]\n",
    "    # TODO : calculate cosine similarities for each pair above\n",
    "    similarities = \n",
    "    return similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = 'skipgram'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if GPU is available\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "embedding_dim= 64 # you can change, if you want\n",
    "\n",
    "model = SkipGram(len(vocab_to_int), embedding_dim).to(device)\n",
    "# Explain why we can or can't define our loss function in advance based on our needs for this specific problem.\n",
    "# Hint: If we had a static graph, we could define it as an object\n",
    "optimizer = optim.Adam(model.parameters(), lr=3e-3)\n",
    "# model.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_every = 30\n",
    "steps = 0\n",
    "epochs = 5\n",
    "\n",
    "for step in range(epochs * train_length):\n",
    "    # TODO: Fill out sampling and loss function\n",
    "    xi_center = \n",
    "    xi_outer = \n",
    "    xneg_center = \n",
    "    xneg_outer = \n",
    "    x_center = \n",
    "    x_outer = \n",
    "    targets = \n",
    "    steps += 1\n",
    "    x_center_torch, x_outer_torch, targets_torch = [torch.LongTensor(x).to(device) for x in [x_center, x_outer, targets]]\n",
    "    \n",
    "    logits, probs = model(x_center_torch, x_outer_torch, targets_torch)\n",
    "    optimizer.zero_grad()\n",
    "    loss = # apply loss function\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if step % print_every == 0:\n",
    "        # TODO: Evaluate models perfomance based on mean of similarities\n",
    "        embed = \n",
    "        scores = \n",
    "        score_avg = \n",
    "        print(f'eval score: {score_avg:.5f}')\n",
    "    if step % 10000 == 0 and step > 1:\n",
    "        torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the word vectors\n",
    "\n",
    "Below we'll use T-SNE to visualize how our high-dimensional word vectors cluster together. T-SNE is used to project these vectors into two dimensions while preserving local stucture. Check out [this post from Christopher Olah](http://colah.github.io/posts/2014-10-Visualizing-MNIST/) to learn more about T-SNE and other ways to visualize high-dimensional data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting embeddings from the embedding layer of our model, by name\n",
    "embeddings = model.get_emb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz_words = 600\n",
    "tsne = TSNE()\n",
    "embed_tsne = tsne.fit_transform(embeddings[:viz_words, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16, 16))\n",
    "for idx in range(viz_words):\n",
    "    plt.scatter(*embed_tsne[idx, :], color='steelblue')\n",
    "    plt.annotate(int_to_vocab[idx], (embed_tsne[idx, 0], embed_tsne[idx, 1]), alpha=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_words = ['big', 'bigger', 'biggest', 'strong', 'stronger', 'strongest', 'slow', 'slower', 'slowest',\n",
    "                  'clear', 'clearer', 'clearest', 'loud', 'louder', 'loudest', 'dark', 'darker', 'darkest']\n",
    "selected_idxes = [vocab_to_int[wd] for wd in selected_words]\n",
    "lowdim_embeddings = PCA().fit_transform(embeddings)\n",
    "embed_pca = lowdim_embeddings[selected_idxes, :2]\n",
    "fig, ax = plt.subplots(figsize=(16, 16))\n",
    "for i, idx in enumerate(selected_idxes):\n",
    "    plt.scatter(*embed_pca[i, :], color='steelblue')\n",
    "    plt.annotate(selected_words[i], (embed_pca[i, 0], embed_pca[i, 1]), alpha=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
