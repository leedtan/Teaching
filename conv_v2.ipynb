{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This lecture is meant to show how great convolutions are and shows some examples of how they are derived from\n",
    "# Applying some logic to linear regression.\n",
    "# It will be followed up with a numpy session actually implementing these tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import cifar10\n",
    "\n",
    "(trainX, trainy), (testX, testy) = cifar10.load_data()\n",
    "print('Train: X=%s, y=%s' % (trainX.shape, trainy.shape))\n",
    "print('Test: X=%s, y=%s' % (testX.shape, testy.shape))\n",
    "for i in range(9):\n",
    "    plt.subplot(330 + 1 + i)\n",
    "    plt.imshow(trainX[i][:,::-1, :]/255 + np.random.normal(size=trainX[i][:,::-1, :].shape) * 1e-1)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_data = 100\n",
    "train_images = trainX[:n_data]\n",
    "train_labels = pd.get_dummies(trainy[:n_data,0]).values\n",
    "test_images = testX[:n_data]\n",
    "test_labels = pd.get_dummies(testy[:n_data,0]).values\n",
    "n_cls = train_labels.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image augmentation via flip LR and Transpose\n",
    "train_images = np.concatenate((train_images, train_images[:,:,::-1,:]), axis=0)\n",
    "train_labels = np.concatenate((train_labels, train_labels), axis=0)\n",
    "\n",
    "train_images = np.concatenate((train_images, np.swapaxes(train_images, 1, 2)), axis=0)/255\n",
    "train_labels = np.concatenate((train_labels, train_labels), axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "tfph = tf.compat.v1.placeholder\n",
    "tfvar = tf.compat.v1.get_variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "style = 'conv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # FC\n",
    "# w = np.random.randn(n_in, n_hidden)\n",
    "# W * W * 3 * W * W * 3\n",
    "# 9e12\n",
    "\n",
    "# # making the model locally connected\n",
    "# W * W * 3 * (3 * 3 * 3)\n",
    "# 1e8\n",
    "\n",
    "# # using parameter sharing\n",
    "# input: 3 x 3 x 3\n",
    "# output: 3 output filters\n",
    "# 81 parameters\n",
    "# realistically, since we have so few parameters, we wouldnt only learn 3 filters, maybe we learn 1000 filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf.compat.v1.reset_default_graph()\n",
    "if style == 'fc':\n",
    "    img_ph = tfph(shape = (None, 32*32*3), dtype = tf.float32)\n",
    "else:\n",
    "    img_ph = tfph(shape = (None, 32, 32, 3), dtype = tf.float32)\n",
    "features = img_ph\n",
    "for i, size in enumerate([64, 64, 64, 64, 128, 128, 128]):\n",
    "    if style == 'conv':\n",
    "        if i in [0, 4]:\n",
    "            features = tf.compat.v1.layers.conv2d(features, size, (3, 3), padding = 'SAME', strides = (2, 2))\n",
    "        else:\n",
    "            residual = tf.compat.v1.layers.conv2d(features, size, (3, 3), padding = 'SAME')\n",
    "            residual = tf.nn.leaky_relu(residual)\n",
    "            residual = tf.compat.v1.layers.conv2d(residual, size, (3, 3), padding = 'SAME')\n",
    "            features = residual + features\n",
    "    if style == 'lc':\n",
    "        features = tf.keras.layers.LocallyConnected2D(3, (3, 3), padding = 'valid')(features)\n",
    "    if style == 'fc':\n",
    "        features = tf.compat.v1.layers.dense(features, 64 * 64 * 1)\n",
    "    features = tf.nn.leaky_relu(features)\n",
    "if style != 'fc':\n",
    "    linear_features = tf.keras.layers.Flatten()(features)\n",
    "else:\n",
    "    linear_features = features\n",
    "linear_features = tf.compat.v1.layers.Dense(128)(linear_features)\n",
    "linear_features = tf.nn.leaky_relu(linear_features)\n",
    "\n",
    "\n",
    "residual = tf.compat.v1.layers.Dense(128)(linear_features)\n",
    "residual = tf.nn.leaky_relu(residual)\n",
    "residual = tf.compat.v1.layers.Dense(128)(residual)\n",
    "linear_features = residual + linear_features\n",
    "\n",
    "linear_features = tf.nn.leaky_relu(linear_features)\n",
    "yhat_raw = tf.compat.v1.layers.Dense(n_cls)(linear_features)\n",
    "yhat = tf.nn.softmax(yhat_raw, axis=1)\n",
    "y_true = tfph(shape = (None, n_cls), dtype = tf.float32)\n",
    "ce_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = y_true, logits = yhat_raw, axis=1))\n",
    "reg_loss = tf.reduce_sum([tf.reduce_sum(tf.square(v)) for v in tf.compat.v1.trainable_variables()])\n",
    "loss = ce_loss + reg_loss * 1e-6 # For now, disabling regularization.\n",
    "opt = tf.compat.v1.train.AdamOptimizer().minimize(loss)\n",
    "# opt = tf.compat.v1.train.MomentumOptimizer(1e-3, .9, use_nesterov=True).minimize(loss)\n",
    "sess = tf.compat.v1.Session()\n",
    "sess.run(tf.compat.v1.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_fd = {img_ph: train_images, y_true: train_labels}\n",
    "if style == 'fc':\n",
    "    batch_fd[img_ph] = train_images.reshape(-1, 32*32*3)\n",
    "\n",
    "valid_fd = {img_ph: test_images}\n",
    "tst_argmax = test_labels.argmax(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_argmax = train_labels.argmax(1)\n",
    "pred_argmax = sess.run(yhat, batch_fd).argmax(1)\n",
    "acc = (pred_argmax == train_argmax).mean()\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "accs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grads_to_input = tf.gradients(loss, img_ph)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(50000):\n",
    "    samples = np.random.choice(train_images.shape[0], size = 32, replace = False)\n",
    "#     _, cur_loss = sess.run(\n",
    "#         [opt, loss], \n",
    "#         {img_ph: train_images[samples] +np.random.normal(size=train_images[samples].shape)*1e-1,\n",
    "#                                          y_true: train_labels[samples]})\n",
    "    img = train_images[samples]\n",
    "    grad = sess.run(grads_to_input, {img_ph: img,y_true: train_labels[samples]})\n",
    "#     if np.sqrt(np.square(grad).sum(1).sum(1).sum(1)).min() <= 0:\n",
    "#         breakpoint()\n",
    "    grad_norm = np.sqrt(np.square(grad).sum(1).sum(1).sum(1))[:,None,None,None]\n",
    "    grad_norm = np.clip(grad_norm, 1e-8, np.inf)\n",
    "    img = img + 0.1 * grad/grad_norm\n",
    "    _, cur_loss = sess.run(\n",
    "        [opt, loss], {img_ph: img,y_true: train_labels[samples]})\n",
    "    if i % 50 == 0:\n",
    "        print(cur_loss)\n",
    "        predictions = sess.run(yhat, valid_fd)\n",
    "        pred_argmax = predictions.argmax(1)\n",
    "        acc = (pred_argmax == tst_argmax).mean()\n",
    "        accs.append(acc)\n",
    "        print('test acc', acc)\n",
    "plt.plot(accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(pred_argmax, tst_argmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.heatmap(confusion_matrix(pred_argmax, tst_argmax), annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
